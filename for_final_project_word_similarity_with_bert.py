# -*- coding: utf-8 -*-
"""for_final_project_word_similarity_with_BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XcsdHN6StzvjcdRjSAtn9I51Ghc_MhLv

## **Import necessary Python libraries and modules**
"""

!pip install transformers

"""Then we will import the DistilBertModel and DistilBertTokenizerFast from the Hugging Face `transformers` library. We will also import a handful of other Python libraries and modules."""

# For BERT
from transformers import DistilBertTokenizerFast, DistilBertModel

# For data manipulation and analysis
import pandas as pd
pd.options.display.max_colwidth = 200
import numpy as np
from sklearn.decomposition import PCA

# For interactive data visualization
import altair as alt

"""## **Load text dataset**"""

from google.colab import drive
drive.mount('/content/drive')

# four words to look (to be updated oncce TM is done running): economy, war, immigration, jobs, peace, guns, taxes


df1 = pd.read_csv('/content/drive/MyDrive/QTM340 Final Project/Copy of presidential_speeches_1.csv', encoding = 'utf-8', sep = '+')
df2 = pd.read_csv('/content/drive/MyDrive/QTM340 Final Project/Copy of presidential_speeches_2.csv', encoding = 'utf-8', sep = '+')
df3 = pd.read_csv('/content/drive/MyDrive/QTM340 Final Project/Copy of presidential_speeches_3.csv', encoding = 'utf-8', sep = '+')

df = pd.concat([df1, df2, df3])
df = df.reset_index(drop=True)
df

# Show 5 random rows
#df.sample(5)

#number of total documents
len(df)

"""Let's check to see which authors show up the most in this dataset to get a sense of its contours:"""

df['speaker'].value_counts()[:20]

#include in the paper

"""**What potential biases in the dataset can you detect from the value counts above?**"""

# More modern presidents, way more speeches from certain administrations

# making the dates

df['datetime'] = pd.to_datetime(df['date'])

# create a list of our conditions
conditions = [
    (df['datetime'] < pd.Timestamp(1993, 1, 20)),
    ((df['datetime'] >= pd.Timestamp(1993, 1, 20)) & (df['datetime'] < pd.Timestamp(2009, 1, 20))), # george bush & clinton
    (df['datetime'] >= pd.Timestamp(2009, 1, 20)) # obama, trump, biden
    ]

# create a list of the values we want to assign for each condition
values = ['before_1993', 'between_1993_2009', 'between_2009_2021']

# create a new column and use np.select to assign values to it using our lists as arguments
df['generation'] = np.select(conditions, values)

"""Let's also check to see what time periods show up the most in this dataset to get a sense of its contours:"""

# Sort values, then create a histogram, and define the size of the figure

df['generation'].sort_values().hist(figsize=(15, 5))

# much better analysis for recent years because we have more documents for these

"""## **Sample text dataset**"""

# Filter the DataFrame for only a given time period, then randomly sample rows
# splitting into 3 dates because of quantity 

#we're not going to take a sample for speeches because we are already limited by the size of our df
#sike took a sample cause when i ran without one it crashed

#sample is total # of documents in each cohort divided by 10

#pre_1993_sample = df[df['generation'] == '1776_1993'].sample(319)
#from_1993_2009_sample = df[df['generation'] == '1993_2009'].sample(677)
#from_2009_present_sample = df[df['generation'] == '2009_2021'].sample(536)

# Merge these random samples into a new DataFrame
#speeches_df = pd.concat([pre_1993_sample, from_1993_2009_sample, from_2009_present_sample])

#new DF but without sampling
speeches_df = df

#ct. of documents in each subdivided date
speeches_df['generation'].value_counts()
#should be 15320 for full set, or ~1530 for sample

#df['generation'].value_counts()

#list of speeches
df_text = speeches_df['text'].tolist()

#looking at speeches
len(df_text)

print(df_text[0])

"""## **Encode/tokenize text data for BERT**

Next we need to *encode* (or *tokenize*) our poems into the format that BERT (via Huggingface) will understand. We did this last class as well. 

As before, we'll tokenize the poems with the `tokenizer()` from HuggingFace's `DistilBertTokenizerFast`. Recall what the `tokenizer()` will do:

1. Truncate the texts if they're more than 512 tokens or pad them if they're fewer than 512 tokens. If a word is not in BERT's vocabulary, it will be broken up into smaller "word pieces," demarcated by a `##`.

2. Add in special tokens to help BERT:
    - [CLS] — Start token of every document
    - [SEP] — Separator between each sentence 
    - [PAD] — Padding at the end of the document as many times as necessary, up to 512 tokens
    - &#35;&#35; — Start of a "word piece"

Here we will load `DistilBertTokenizerFast` from HuggingFace library, which will help us transform and encode the texts so they can be used with BERT.
"""

from transformers import DistilBertTokenizerFast

tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

"""The `tokenizer()` will break word tokens into word pieces, truncate to 512 tokens, and add padding and special BERT tokens."""

tokenized_speeches = tokenizer(df_text, truncation=True, padding=True, return_tensors="pt")

#tokenized speech
' '.join(tokenized_speeches[0].tokens)

"""<br><br>

## **Load pre-trained BERT model**

Here we will load a pre-trained BERT model. To speed things up we will use a GPU, but using GPU involves a few extra steps.
The command `.to("cuda")` moves data from regular memory to the GPU's memory.
"""

from transformers import DistilBertModel

model = DistilBertModel.from_pretrained('distilbert-base-uncased').to("cuda")

"""## **Get BERT word embeddings for each document in a collection**

To get word embeddings for all the words in our collection, we will use a basic `for` loop.

The process is as follows: for each poem in our list `poetry_texts`, we will tokenize the poem, and we will extract the vocabulary word ID for each word/token in the poem (to use for later reference). Then we will run the tokenized poem through the BERT model and extract the vectors for each word/token in the poem.

We thus create two big lists for all the poems in our collection — `doc_word_ids` and `doc_word_vectors`.
"""

# List of vocabulary word IDs for all the words in eachs speech
doc_word_ids = []
# List of word vectors ""
doc_word_vectors = []

# slicing speeches first (0th) and last (-1) special BERT tokens
start_of_words = 1
end_of_words = -1

# indexing the 0th or first document, which will be the only document
first_document = 0

for i, speech in enumerate(df_text):
  
    # tokenize each speech with the DistilBERT Tokenizer
    inputs = tokenizer(speech, return_tensors="pt", truncation=True, padding=True)

    # extractinf vocabulary word ids for all the words, ignoring  first and last special BERT tokens
    # also convert from a Pytorch tensor to a numpy array
    doc_word_ids.append(inputs.input_ids[first_document].numpy()[start_of_words:end_of_words])

    # send the tokenized speech to the GPU (model is there but not speech)
    inputs.to("cuda")
    # run speech through model
    outputs = model(**inputs)

    # We take every element from the first or 0th document, from the 2nd to the 2nd to last position
    # Grabbing the last layer is one way of getting token vectors. There are different ways to get vectors with different pros and cons
    doc_word_vectors.append(outputs.last_hidden_state[first_document,start_of_words:end_of_words,:].detach().cpu().numpy())

"""Confirm that we have the same number of documents for both the tokens and the vectors:"""

len(doc_word_ids), len(doc_word_vectors)

doc_word_ids[0], doc_word_vectors[0]

# Take a look at the number of vectors for one of the words
len(doc_word_vectors[0])

"""## **Concatenate all word IDs/vectors for all documents**

Each element of these lists contains all the tokens/vectors for one document. But we want to concatenate them into two giant collections for ease of use.
"""

all_word_ids = np.concatenate(doc_word_ids)
all_word_vectors = np.concatenate(doc_word_vectors, axis=0)

"""## **Computing Cosine Similarity**"""

#Here's where we compute the cosine similarity of the vectors. 
#Because cosine similarity measures the angle between vectors but ignores their length, 
#we can speed this computation up by setting all the speech vectors to have length 1.0.
# Calculating the length of each vector (Pythagorean theorem)
row_norms = np.sqrt(np.sum(all_word_vectors ** 2, axis=1))
# Dividing every vector by its length
all_word_vectors /= row_norms[:,np.newaxis]

#calc cosine similarity
def get_nearest(query_vector, n=20):
  cosines = all_word_vectors.dot(query_vector)
  ordering = np.flip(np.argsort(cosines))
  return ordering[:n]

"""## **Helpful functions** 

**Find all word positions in a collection**

This function uses the array `all_word_ids` to find all the places, or *positions*, in the collection where a word appears.

We'll do this with the `tokenizer.vocab` attribute, finding the word's vocab ID in BERT and then checking to see where/how many times this ID occurs in `all_word_ids`.
"""

def get_word_positions(words):
  
  """This function accepts a list of words, rather than a single word"""

  # Get word/vocabulary ID from BERT for each word
  word_ids = [tokenizer.vocab[word] for word in words]

  # Find all the positions where the words occur in the collection
  word_positions = np.where(np.isin(all_word_ids, word_ids))[0]

  return word_positions

#check to see all the places where the word "war" appears in the collection.
get_word_positions(["war"])

"""**Use get_word_positions to find word from word position**

The above function allow us to determine all the positions where the word "bank" appears in the collection. But it would be more helpful to us as humans to know the actual words that appear in context around it. To find these context words, we have to convert position IDs back into words, like so:
"""

# Here we create an array so that we can go backwards from numeric token IDs to words
word_lookup = np.empty(tokenizer.vocab_size, dtype="O")

for word, index in tokenizer.vocab.items():
    word_lookup[index] = word

"""Now we can use `word_lookup` to find a word based on its position in the collection. Let's first see what the output of a basic lookup operation looks like."""

word_positions = get_word_positions(["war"])

for word_position in word_positions:
  print(word_position, word_lookup[all_word_ids[word_position]])

"""But we're not done yet. We can modify the above code to look for the 3 words that come before "bank" and the 3 words that come after it."""

word_positions = get_word_positions(["war"])

for word_position in word_positions:

  # Slice 3 words before "bank"
  start_pos = word_position - 3
  # Slice 3 words after "bank"
  end_pos = word_position + 4

  context_words = word_lookup[all_word_ids[start_pos:end_pos]]
  # Join the words together
  context_words = ' '.join(context_words)
  print(word_position, context_words)

"""Now that we've tested out this approach, let's make some functions that will help us get the context words around a certain word position for whatever size window (certain number of words before and after) that we want.

The first function `get_context()` will simply return the tokens without cleaning them, and the second function `get_context_clean()` will return the tokens in a more readable fashion. (Note the return of regex!) 
"""

def get_context(word_id, window_size=10):
  
  """Simply get the tokens that occur before and after word position"""

  start_pos = max(0, word_id - window_size) # The token where we will start the context view
  end_pos = min(word_id + window_size + 1, len(all_word_ids)) # The token where we will end the context view

  # Make a list called tokens and use word_lookup to get the words for given token IDs from starting position up to the keyword
  tokens = [word_lookup[word] for word in all_word_ids[start_pos:end_pos] ]
  
  context_words = " ".join(tokens)

  return context_words

import re

def get_context_clean(word_id, window_size=10):
  
  """Get the tokens that occur before and after word position AND make them more readable"""

  keyword = word_lookup[all_word_ids[word_id]]
  start_pos = max(0, word_id - window_size) # The token where we will start the context view
  end_pos = min(word_id + window_size + 1, len(all_word_ids)) # The token where we will end the context view

  # Make a list called tokens and use word_lookup to get the words for given token IDs from starting position up to the keyword
  tokens = [word_lookup[word] for word in all_word_ids[start_pos:end_pos] ]
  
  # Make wordpieces slightly more readable
  # This is probably not the most efficient way to clean and correct for weird spacing
  context_words = " ".join(tokens)
  context_words = re.sub(r'\s+([##])', r'\1', context_words)
  context_words = re.sub(r'##', r'', context_words)
  context_words = re.sub('\s+\'s', '\'s', context_words)
  context_words = re.sub('\s+\'d', '\'d', context_words)
  context_words = re.sub('\s\'er', '\'er', context_words)
  context_words = re.sub(r'\s+([-,:?.!;])', r'\1', context_words)
  context_words = re.sub(r'([-\'"])\s+', r'\1', context_words)
  context_words = re.sub('\s+\'s', '\'s', context_words)
  context_words = re.sub('\s+\'d', '\'d', context_words)

  # Bold the keyword by putting asterisks around it
  if keyword in context_words:
    context_words = re.sub(f"\\b{keyword}\\b", f"**{keyword}**", context_words)
    context_words = re.sub(f"\\b({keyword}[esdtrlying]+)\\b", fr"**\1**", context_words)

  return context_words

"""## **Examining most similar vectors**

At long last, we're ready to perform our similarity comparison. First we need to decide which particular instance of "bank" we want to examine. Let's print out some instances of "bank" in our poetry corpus to see if one seems interesting to us. 


"""

# load some libraries for displaying text nicely
from IPython.display import Markdown, display

def print_md(string):
    display(Markdown(string))

# print out instances of "bank" in the corpus
word_positions = get_word_positions(['immigrants'])

for word_position in word_positions:

  print_md(f"<br> {word_position}: {get_context_clean(word_position)} <br>")

"""Now we can pick any of these keyword positions to compare to."""

keyword_position = 349532 # can be replaced with any of the above

"""And at long last, display the most similar vectors"""

contexts = [get_context_clean(token_id) for token_id in get_nearest(all_word_vectors[keyword_position,:])]

for context in contexts:
  print_md(context)

"""## **Reduce word vectors via PCA and plot them**"""

from sklearn.decomposition import PCA

word_positions = get_word_positions(["war"])

pca = PCA(n_components=2)

pca.fit(all_word_vectors[word_positions,:].T)

"""Let's now make a list of all of the context views for our keyword so that we can associate them with their PCA score:"""

word_positions = get_word_positions(["war"])

keyword_contexts = []
keyword_contexts_tokens = []

for position in word_positions:

  keyword_contexts.append(get_context_clean(position))
  keyword_contexts_tokens.append(get_context(position))

"""Then, for convenience, we will put these PCA results into a Pandas DataFrame, which will use to generate an interactive plot."""

df = pd.DataFrame({"x": pca.components_[0,:], "y": pca.components_[1,:],
                   "context": keyword_contexts, "tokens": keyword_contexts_tokens})
df.head()

"""## **Match context with original text and metadata**

It's helpful (and fun!) to know where each instance of a word actually comes from — which poem, which poet, which time period, which Public-Domain-Poetry.com web page. The easiest method we've found for matching a bit of context with its original poem and metdata is to 1) add a tokenized version of each poem to our original Pandas Dataframe 2) check to see if the context shows up in a poem 3) and if so, grab the original poem and metadata.
"""

# Tokenize all 
tokenized_speeches = tokenizer(df_text, truncation=True, padding=True, return_tensors="pt")

# Get a list of all the tokens 
all_tokenized_speeches = []
for i in range(len(tokenized_speeches['input_ids'])):
  all_tokenized_speeches.append(' '.join(tokenized_speeches[i].tokens))

# Add them to the original DataFrame
speeches_df['tokens'] = all_tokenized_speeches

speeches_df.head(2)

def find_original_speech(rows):

  text = rows['tokens'].replace('**', '')
  text = text[55:70]

  if speeches_df['tokens'].str.contains(text, regex=False).any() == True :
    row = speeches_df[speeches_df['tokens'].str.contains(text, regex=False)].values[0]
    title, speaker, generation, citation = row[0], row[2], row[6], row[4]
    return title, speaker, generation, citation
  else:
    return None, None, None, None

df[['title', 'speaker', 'generation', 'citation']] = df.apply(find_original_speech, axis='columns', result_type='expand')

df

"""## **Plot word embeddings**

Lastly, we will plot the words vectors from this DataFrame with the Python data viz library [Altair](https://altair-viz.github.io/gallery/scatter_tooltips.html).
"""

import altair as alt

alt.Chart(df,title="Word Similarity: War").mark_circle(size=200).encode(
    alt.X('x',
        scale=alt.Scale(zero=False)
    ), y="y",
    # If you click a point, take you to the URL link 
    #href="link",
    # The categories that show up in the hover tooltip
    tooltip=['title', 'speaker', 'generation', 'citation']
    ).interactive().properties(
    width=500,
    height=500
)

"""## **Plot word embeddings from keywords (all at once!)**"""

# List of keywords that you want to compare
# we are doing top five words in each topic for five topics (based on which we found different) 
# based on T0 - military topic
keywords = ['military', 'service', 'families', 'nation', 'men']

# How to color the points in the plot. The other option is "period" for time period
color_by = 'word'
# Get all word positions
word_positions = get_word_positions(keywords)

# Get all contexts around the words
keyword_contexts = []
keyword_contexts_tokens = []
words = []

for position in word_positions:
  words.append(word_lookup[all_word_ids[position]])
  keyword_contexts.append(get_context_clean(position))
  keyword_contexts_tokens.append(get_context(position))

# Reduce word vectors with PCA
pca = PCA(n_components=2)
pca.fit(all_word_vectors[word_positions,:].T)

# Make a DataFrame with PCA results
df = pd.DataFrame({"x": pca.components_[0,:], "y": pca.components_[1,:],
                   "context": keyword_contexts, "tokens": keyword_contexts_tokens, "word": words})
# Match original text and metadata
df[['title', 'speaker', 'generation', 'citation']] = df.apply(find_original_speech, axis='columns', result_type='expand')

# Rename columns so that the context shows up as the "title" in the tooltip (bigger and bolded)
df = df.rename(columns={'title': 'title', 'context': 'speech_title'})

# Make the plot
alt.Chart(df, title=f"Word Similarity: {', '.join(keywords).title()}").mark_circle(size=200).encode(
    alt.X('x',
        scale=alt.Scale(zero=False)
    ), y="y",
    color= color_by,
    tooltip=['speech_title', 'citation', 'title', 'speaker', 'generation']
    ).interactive().properties(
    width=500,
    height=500
)

# List of keywords that you want to compare
# we are doing top five words in each topic for five topics (based on which we found different) 
# based on T0 - military topic, same as above but by gen
keywords = ['military', 'service', 'families', 'nation', 'men']

# How to color the points in the plot. The other option is "period" for time period
color_by = 'generation'
# Get all word positions
word_positions = get_word_positions(keywords)

# Get all contexts around the words
keyword_contexts = []
keyword_contexts_tokens = []
words = []

for position in word_positions:
  words.append(word_lookup[all_word_ids[position]])
  keyword_contexts.append(get_context_clean(position))
  keyword_contexts_tokens.append(get_context(position))

# Reduce word vectors with PCA
pca = PCA(n_components=2)
pca.fit(all_word_vectors[word_positions,:].T)

# Make a DataFrame with PCA results
df = pd.DataFrame({"x": pca.components_[0,:], "y": pca.components_[1,:],
                   "context": keyword_contexts, "tokens": keyword_contexts_tokens, "word": words})
# Match original text and metadata
df[['title', 'speaker', 'generation', 'citation']] = df.apply(find_original_speech, axis='columns', result_type='expand')

# Rename columns so that the context shows up as the "title" in the tooltip (bigger and bolded)
df = df.rename(columns={'title': 'title', 'context': 'speech_title'})

# Make the plot
alt.Chart(df, title=f"Word Similarity: {', '.join(keywords).title()}").mark_circle(size=200).encode(
    alt.X('x',
        scale=alt.Scale(zero=False)
    ), y="y",
    color= color_by,
    tooltip=['speech_title', 'citation', 'title', 'speaker', 'generation']
    ).interactive().properties(
    width=500,
    height=500
)

# List of keywords that you want to compare
# we are now doing one word at a time to compare over generation with a more clear output
# t0 w1
keywords = ['military']

# How to color the points in the plot. The other option is "period" for time period
color_by = 'generation'

# Get all word positions
word_positions = get_word_positions(keywords)

# Get all contexts around the words
keyword_contexts = []
keyword_contexts_tokens = []
words = []

for position in word_positions:
  words.append(word_lookup[all_word_ids[position]])
  keyword_contexts.append(get_context_clean(position))
  keyword_contexts_tokens.append(get_context(position))

# Reduce word vectors with PCA
pca = PCA(n_components=2)
pca.fit(all_word_vectors[word_positions,:].T)

# Make a DataFrame with PCA results
df = pd.DataFrame({"x": pca.components_[0,:], "y": pca.components_[1,:],
                   "context": keyword_contexts, "tokens": keyword_contexts_tokens, "word": words})
# Match original text and metadata
df[['title', 'speaker', 'generation', 'citation']] = df.apply(find_original_speech, axis='columns', result_type='expand')

# Rename columns so that the context shows up as the "title" in the tooltip (bigger and bolded)
df = df.rename(columns={'title': 'title', 'context': 'speech_title'})

# Make the plot
alt.Chart(df, title=f"Word Similarity: {', '.join(keywords).title()}").mark_circle(size=200).encode(
    alt.X('x',
        scale=alt.Scale(zero=False)
    ), y="y",
    color= color_by,
    tooltip=['speech_title', 'citation', 'title', 'speaker', 'generation']
    ).interactive().properties(
    width=500,
    height=500
)

# List of keywords that you want to compare
# we are now doing one word at a time to compare over generation with a more clear output
#t0 w2
keywords = ['service']

# How to color the points in the plot. The other option is "period" for time period
color_by = 'generation'

# Get all word positions
word_positions = get_word_positions(keywords)

# Get all contexts around the words
keyword_contexts = []
keyword_contexts_tokens = []
words = []

for position in word_positions:
  words.append(word_lookup[all_word_ids[position]])
  keyword_contexts.append(get_context_clean(position))
  keyword_contexts_tokens.append(get_context(position))

# Reduce word vectors with PCA
pca = PCA(n_components=2)
pca.fit(all_word_vectors[word_positions,:].T)

# Make a DataFrame with PCA results
df = pd.DataFrame({"x": pca.components_[0,:], "y": pca.components_[1,:],
                   "context": keyword_contexts, "tokens": keyword_contexts_tokens, "word": words})
# Match original text and metadata
df[['title', 'speaker', 'generation', 'citation']] = df.apply(find_original_speech, axis='columns', result_type='expand')

# Rename columns so that the context shows up as the "title" in the tooltip (bigger and bolded)
df = df.rename(columns={'title': 'title', 'context': 'speech_title'})

# Make the plot
alt.Chart(df, title=f"Word Similarity: {', '.join(keywords).title()}").mark_circle(size=200).encode(
    alt.X('x',
        scale=alt.Scale(zero=False)
    ), y="y",
    color= color_by,
    tooltip=['speech_title', 'citation', 'title', 'speaker', 'generation']
    ).interactive().properties(
    width=500,
    height=500
)

# List of keywords that you want to compare
# we are now doing one word at a time to compare over generation with a more clear output
# t0 w3
keywords = ['families']

# How to color the points in the plot. The other option is "period" for time period
color_by = 'generation'

# Get all word positions
word_positions = get_word_positions(keywords)

# Get all contexts around the words
keyword_contexts = []
keyword_contexts_tokens = []
words = []

for position in word_positions:
  words.append(word_lookup[all_word_ids[position]])
  keyword_contexts.append(get_context_clean(position))
  keyword_contexts_tokens.append(get_context(position))

# Reduce word vectors with PCA
pca = PCA(n_components=2)
pca.fit(all_word_vectors[word_positions,:].T)

# Make a DataFrame with PCA results
df = pd.DataFrame({"x": pca.components_[0,:], "y": pca.components_[1,:],
                   "context": keyword_contexts, "tokens": keyword_contexts_tokens, "word": words})
# Match original text and metadata
df[['title', 'speaker', 'generation', 'citation']] = df.apply(find_original_speech, axis='columns', result_type='expand')

# Rename columns so that the context shows up as the "title" in the tooltip (bigger and bolded)
df = df.rename(columns={'title': 'title', 'context': 'speech_title'})

# Make the plot
alt.Chart(df, title=f"Word Similarity: {', '.join(keywords).title()}").mark_circle(size=200).encode(
    alt.X('x',
        scale=alt.Scale(zero=False)
    ), y="y",
    color= color_by,
    tooltip=['speech_title', 'citation', 'title', 'speaker', 'generation']
    ).interactive().properties(
    width=500,
    height=500
)

# List of keywords that you want to compare
# we are doing top five words in each topic for five topics (based on which we found different) 
# based on T4 - health care topic by word
keywords = ['health', 'care', 'insurance', 'medicare', 'americans'] 

# How to color the points in the plot. The other option is "period" for time period
color_by = 'word'
# Get all word positions
word_positions = get_word_positions(keywords)

# Get all contexts around the words
keyword_contexts = []
keyword_contexts_tokens = []
words = []

for position in word_positions:
  words.append(word_lookup[all_word_ids[position]])
  keyword_contexts.append(get_context_clean(position))
  keyword_contexts_tokens.append(get_context(position))

# Reduce word vectors with PCA
pca = PCA(n_components=2)
pca.fit(all_word_vectors[word_positions,:].T)

# Make a DataFrame with PCA results
df = pd.DataFrame({"x": pca.components_[0,:], "y": pca.components_[1,:],
                   "context": keyword_contexts, "tokens": keyword_contexts_tokens, "word": words})
# Match original text and metadata
df[['title', 'speaker', 'generation', 'citation']] = df.apply(find_original_speech, axis='columns', result_type='expand')

# Rename columns so that the context shows up as the "title" in the tooltip (bigger and bolded)
df = df.rename(columns={'title': 'title', 'context': 'speech_title'})

# Make the plot
alt.Chart(df, title=f"Word Similarity: {', '.join(keywords).title()}").mark_circle(size=200).encode(
    alt.X('x',
        scale=alt.Scale(zero=False)
    ), y="y",
    color= color_by,
    tooltip=['speech_title', 'citation', 'title', 'speaker', 'generation']
    ).interactive().properties(
    width=500,
    height=500
)

# List of keywords that you want to compare
# we are doing top five words in each topic for five topics (based on which we found different) 
# based on T4 - health care topic by gen
keywords = ['health', 'care', 'insurance', 'medicare', 'americans'] 

# How to color the points in the plot. The other option is "period" for time period
color_by = 'generation'
# Get all word positions
word_positions = get_word_positions(keywords)

# Get all contexts around the words
keyword_contexts = []
keyword_contexts_tokens = []
words = []

for position in word_positions:
  words.append(word_lookup[all_word_ids[position]])
  keyword_contexts.append(get_context_clean(position))
  keyword_contexts_tokens.append(get_context(position))

# Reduce word vectors with PCA
pca = PCA(n_components=2)
pca.fit(all_word_vectors[word_positions,:].T)

# Make a DataFrame with PCA results
df = pd.DataFrame({"x": pca.components_[0,:], "y": pca.components_[1,:],
                   "context": keyword_contexts, "tokens": keyword_contexts_tokens, "word": words})
# Match original text and metadata
df[['title', 'speaker', 'generation', 'citation']] = df.apply(find_original_speech, axis='columns', result_type='expand')

# Rename columns so that the context shows up as the "title" in the tooltip (bigger and bolded)
df = df.rename(columns={'title': 'title', 'context': 'speech_title'})

# Make the plot
alt.Chart(df, title=f"Word Similarity: {', '.join(keywords).title()}").mark_circle(size=200).encode(
    alt.X('x',
        scale=alt.Scale(zero=False)
    ), y="y",
    color= color_by,
    tooltip=['speech_title', 'citation', 'title', 'speaker', 'generation']
    ).interactive().properties(
    width=500,
    height=500
)

# List of keywords that you want to compare
# we are doing top five words in each topic for five topics (based on which we found different) 
# based on T4 w1
keywords = ['health']

# How to color the points in the plot. The other option is "period" for time period
color_by = 'generation'
# Get all word positions
word_positions = get_word_positions(keywords)

# Get all contexts around the words
keyword_contexts = []
keyword_contexts_tokens = []
words = []

for position in word_positions:
  words.append(word_lookup[all_word_ids[position]])
  keyword_contexts.append(get_context_clean(position))
  keyword_contexts_tokens.append(get_context(position))

# Reduce word vectors with PCA
pca = PCA(n_components=2)
pca.fit(all_word_vectors[word_positions,:].T)

# Make a DataFrame with PCA results
df = pd.DataFrame({"x": pca.components_[0,:], "y": pca.components_[1,:],
                   "context": keyword_contexts, "tokens": keyword_contexts_tokens, "word": words})
# Match original text and metadata
df[['title', 'speaker', 'generation', 'citation']] = df.apply(find_original_speech, axis='columns', result_type='expand')

# Rename columns so that the context shows up as the "title" in the tooltip (bigger and bolded)
df = df.rename(columns={'title': 'title', 'context': 'speech_title'})

# Make the plot
alt.Chart(df, title=f"Word Similarity: {', '.join(keywords).title()}").mark_circle(size=200).encode(
    alt.X('x',
        scale=alt.Scale(zero=False)
    ), y="y",
    color= color_by,
    tooltip=['speech_title', 'citation', 'title', 'speaker', 'generation']
    ).interactive().properties(
    width=500,
    height=500
)

# List of keywords that you want to compare
# we are doing top five words in each topic for five topics (based on which we found different) 
# based on T4 w2
keywords = ['care']

# How to color the points in the plot. The other option is "period" for time period
color_by = 'generation'
# Get all word positions
word_positions = get_word_positions(keywords)

# Get all contexts around the words
keyword_contexts = []
keyword_contexts_tokens = []
words = []

for position in word_positions:
  words.append(word_lookup[all_word_ids[position]])
  keyword_contexts.append(get_context_clean(position))
  keyword_contexts_tokens.append(get_context(position))

# Reduce word vectors with PCA
pca = PCA(n_components=2)
pca.fit(all_word_vectors[word_positions,:].T)

# Make a DataFrame with PCA results
df = pd.DataFrame({"x": pca.components_[0,:], "y": pca.components_[1,:],
                   "context": keyword_contexts, "tokens": keyword_contexts_tokens, "word": words})
# Match original text and metadata
df[['title', 'speaker', 'generation', 'citation']] = df.apply(find_original_speech, axis='columns', result_type='expand')

# Rename columns so that the context shows up as the "title" in the tooltip (bigger and bolded)
df = df.rename(columns={'title': 'title', 'context': 'speech_title'})

# Make the plot
alt.Chart(df, title=f"Word Similarity: {', '.join(keywords).title()}").mark_circle(size=200).encode(
    alt.X('x',
        scale=alt.Scale(zero=False)
    ), y="y",
    color= color_by,
    tooltip=['speech_title', 'citation', 'title', 'speaker', 'generation']
    ).interactive().properties(
    width=500,
    height=500
)

# List of keywords that you want to compare
# we are doing top five words in each topic for five topics (based on which we found different) 
# based on T4 w3
keywords = ['insurance']

# How to color the points in the plot. The other option is "period" for time period
color_by = 'generation'
# Get all word positions
word_positions = get_word_positions(keywords)

# Get all contexts around the words
keyword_contexts = []
keyword_contexts_tokens = []
words = []

for position in word_positions:
  words.append(word_lookup[all_word_ids[position]])
  keyword_contexts.append(get_context_clean(position))
  keyword_contexts_tokens.append(get_context(position))

# Reduce word vectors with PCA
pca = PCA(n_components=2)
pca.fit(all_word_vectors[word_positions,:].T)

# Make a DataFrame with PCA results
df = pd.DataFrame({"x": pca.components_[0,:], "y": pca.components_[1,:],
                   "context": keyword_contexts, "tokens": keyword_contexts_tokens, "word": words})
# Match original text and metadata
df[['title', 'speaker', 'generation', 'citation']] = df.apply(find_original_speech, axis='columns', result_type='expand')

# Rename columns so that the context shows up as the "title" in the tooltip (bigger and bolded)
df = df.rename(columns={'title': 'title', 'context': 'speech_title'})

# Make the plot
alt.Chart(df, title=f"Word Similarity: {', '.join(keywords).title()}").mark_circle(size=200).encode(
    alt.X('x',
        scale=alt.Scale(zero=False)
    ), y="y",
    color= color_by,
    tooltip=['speech_title', 'citation', 'title', 'speaker', 'generation']
    ).interactive().properties(
    width=500,
    height=500
)

# List of keywords that you want to compare
# we are doing top five words in each topic for five topics (based on which we found different) 
# based on T3, crime topic
keywords = ['law', 'crime', 'police', 'drug', 'enforcement']

# How to color the points in the plot. The other option is "period" for time period
color_by = 'word'
# Get all word positions
word_positions = get_word_positions(keywords)

# Get all contexts around the words
keyword_contexts = []
keyword_contexts_tokens = []
words = []

for position in word_positions:
  words.append(word_lookup[all_word_ids[position]])
  keyword_contexts.append(get_context_clean(position))
  keyword_contexts_tokens.append(get_context(position))

# Reduce word vectors with PCA
pca = PCA(n_components=2)
pca.fit(all_word_vectors[word_positions,:].T)

# Make a DataFrame with PCA results
df = pd.DataFrame({"x": pca.components_[0,:], "y": pca.components_[1,:],
                   "context": keyword_contexts, "tokens": keyword_contexts_tokens, "word": words})
# Match original text and metadata
df[['title', 'speaker', 'generation', 'citation']] = df.apply(find_original_speech, axis='columns', result_type='expand')

# Rename columns so that the context shows up as the "title" in the tooltip (bigger and bolded)
df = df.rename(columns={'title': 'title', 'context': 'speech_title'})

# Make the plot
alt.Chart(df, title=f"Word Similarity: {', '.join(keywords).title()}").mark_circle(size=200).encode(
    alt.X('x',
        scale=alt.Scale(zero=False)
    ), y="y",
    color= color_by,
    tooltip=['speech_title', 'citation', 'title', 'speaker', 'generation']
    ).interactive().properties(
    width=500,
    height=500
)

# List of keywords that you want to compare
# we are doing top five words in each topic for five topics (based on which we found different) 
# based on T3, crime topic by gen
keywords = ['law', 'crime', 'police', 'drug', 'enforcement']

# How to color the points in the plot. The other option is "period" for time period
color_by = 'generation'
# Get all word positions
word_positions = get_word_positions(keywords)

# Get all contexts around the words
keyword_contexts = []
keyword_contexts_tokens = []
words = []

for position in word_positions:
  words.append(word_lookup[all_word_ids[position]])
  keyword_contexts.append(get_context_clean(position))
  keyword_contexts_tokens.append(get_context(position))

# Reduce word vectors with PCA
pca = PCA(n_components=2)
pca.fit(all_word_vectors[word_positions,:].T)

# Make a DataFrame with PCA results
df = pd.DataFrame({"x": pca.components_[0,:], "y": pca.components_[1,:],
                   "context": keyword_contexts, "tokens": keyword_contexts_tokens, "word": words})
# Match original text and metadata
df[['title', 'speaker', 'generation', 'citation']] = df.apply(find_original_speech, axis='columns', result_type='expand')

# Rename columns so that the context shows up as the "title" in the tooltip (bigger and bolded)
df = df.rename(columns={'title': 'title', 'context': 'speech_title'})

# Make the plot
alt.Chart(df, title=f"Word Similarity: {', '.join(keywords).title()}").mark_circle(size=200).encode(
    alt.X('x',
        scale=alt.Scale(zero=False)
    ), y="y",
    color= color_by,
    tooltip=['speech_title', 'citation', 'title', 'speaker', 'generation']
    ).interactive().properties(
    width=500,
    height=500
)

# List of keywords that you want to compare
# we are doing top five words in each topic for five topics (based on which we found different) 
# based on T3 w1
keywords = ['law']

# How to color the points in the plot. The other option is "period" for time period
color_by = 'generation'
# Get all word positions
word_positions = get_word_positions(keywords)

# Get all contexts around the words
keyword_contexts = []
keyword_contexts_tokens = []
words = []

for position in word_positions:
  words.append(word_lookup[all_word_ids[position]])
  keyword_contexts.append(get_context_clean(position))
  keyword_contexts_tokens.append(get_context(position))

# Reduce word vectors with PCA
pca = PCA(n_components=2)
pca.fit(all_word_vectors[word_positions,:].T)

# Make a DataFrame with PCA results
df = pd.DataFrame({"x": pca.components_[0,:], "y": pca.components_[1,:],
                   "context": keyword_contexts, "tokens": keyword_contexts_tokens, "word": words})
# Match original text and metadata
df[['title', 'speaker', 'generation', 'citation']] = df.apply(find_original_speech, axis='columns', result_type='expand')

# Rename columns so that the context shows up as the "title" in the tooltip (bigger and bolded)
df = df.rename(columns={'title': 'title', 'context': 'speech_title'})

# Make the plot
alt.Chart(df, title=f"Word Similarity: {', '.join(keywords).title()}").mark_circle(size=200).encode(
    alt.X('x',
        scale=alt.Scale(zero=False)
    ), y="y",
    color= color_by,
    tooltip=['speech_title', 'citation', 'title', 'speaker', 'generation']
    ).interactive().properties(
    width=500,
    height=500
)

# List of keywords that you want to compare
# we are doing top five words in each topic for five topics (based on which we found different) 
# based on T3 w2
keywords = ['crime']

# How to color the points in the plot. The other option is "period" for time period
color_by = 'generation'
# Get all word positions
word_positions = get_word_positions(keywords)

# Get all contexts around the words
keyword_contexts = []
keyword_contexts_tokens = []
words = []

for position in word_positions:
  words.append(word_lookup[all_word_ids[position]])
  keyword_contexts.append(get_context_clean(position))
  keyword_contexts_tokens.append(get_context(position))

# Reduce word vectors with PCA
pca = PCA(n_components=2)
pca.fit(all_word_vectors[word_positions,:].T)

# Make a DataFrame with PCA results
df = pd.DataFrame({"x": pca.components_[0,:], "y": pca.components_[1,:],
                   "context": keyword_contexts, "tokens": keyword_contexts_tokens, "word": words})
# Match original text and metadata
df[['title', 'speaker', 'generation', 'citation']] = df.apply(find_original_speech, axis='columns', result_type='expand')

# Rename columns so that the context shows up as the "title" in the tooltip (bigger and bolded)
df = df.rename(columns={'title': 'title', 'context': 'speech_title'})

# Make the plot
alt.Chart(df, title=f"Word Similarity: {', '.join(keywords).title()}").mark_circle(size=200).encode(
    alt.X('x',
        scale=alt.Scale(zero=False)
    ), y="y",
    color= color_by,
    tooltip=['speech_title', 'citation', 'title', 'speaker', 'generation']
    ).interactive().properties(
    width=500,
    height=500
)

# List of keywords that you want to compare
# we are doing top five words in each topic for five topics (based on which we found different) 
# based on T3 w3
keywords = ['police']

# How to color the points in the plot. The other option is "period" for time period
color_by = 'generation'
# Get all word positions
word_positions = get_word_positions(keywords)

# Get all contexts around the words
keyword_contexts = []
keyword_contexts_tokens = []
words = []

for position in word_positions:
  words.append(word_lookup[all_word_ids[position]])
  keyword_contexts.append(get_context_clean(position))
  keyword_contexts_tokens.append(get_context(position))

# Reduce word vectors with PCA
pca = PCA(n_components=2)
pca.fit(all_word_vectors[word_positions,:].T)

# Make a DataFrame with PCA results
df = pd.DataFrame({"x": pca.components_[0,:], "y": pca.components_[1,:],
                   "context": keyword_contexts, "tokens": keyword_contexts_tokens, "word": words})
# Match original text and metadata
df[['title', 'speaker', 'generation', 'citation']] = df.apply(find_original_speech, axis='columns', result_type='expand')

# Rename columns so that the context shows up as the "title" in the tooltip (bigger and bolded)
df = df.rename(columns={'title': 'title', 'context': 'speech_title'})

# Make the plot
alt.Chart(df, title=f"Word Similarity: {', '.join(keywords).title()}").mark_circle(size=200).encode(
    alt.X('x',
        scale=alt.Scale(zero=False)
    ), y="y",
    color= color_by,
    tooltip=['speech_title', 'citation', 'title', 'speaker', 'generation']
    ).interactive().properties(
    width=500,
    height=500
)

# List of keywords that you want to compare
# we are doing top five words in each topic for five topics (based on which we found different) 
# based on t11, economy topic
keywords = ['jobs', 'tax', 'economy', 'business', 'congress'] #omitted going w4, for w7 business

# How to color the points in the plot. The other option is "period" for time period
color_by = 'word'
# Get all word positions
word_positions = get_word_positions(keywords)

# Get all contexts around the words
keyword_contexts = []
keyword_contexts_tokens = []
words = []

for position in word_positions:
  words.append(word_lookup[all_word_ids[position]])
  keyword_contexts.append(get_context_clean(position))
  keyword_contexts_tokens.append(get_context(position))

# Reduce word vectors with PCA
pca = PCA(n_components=2)
pca.fit(all_word_vectors[word_positions,:].T)

# Make a DataFrame with PCA results
df = pd.DataFrame({"x": pca.components_[0,:], "y": pca.components_[1,:],
                   "context": keyword_contexts, "tokens": keyword_contexts_tokens, "word": words})
# Match original text and metadata
df[['title', 'speaker', 'generation', 'citation']] = df.apply(find_original_speech, axis='columns', result_type='expand')

# Rename columns so that the context shows up as the "title" in the tooltip (bigger and bolded)
df = df.rename(columns={'title': 'title', 'context': 'speech_title'})

# Make the plot
alt.Chart(df, title=f"Word Similarity: {', '.join(keywords).title()}").mark_circle(size=200).encode(
    alt.X('x',
        scale=alt.Scale(zero=False)
    ), y="y",
    color= color_by,
    tooltip=['speech_title', 'citation', 'title', 'speaker', 'generation']
    ).interactive().properties(
    width=500,
    height=500
)

# List of keywords that you want to compare
# we are doing top five words in each topic for five topics (based on which we found different) 
# based on t11, economy topic by gen
keywords = ['jobs', 'tax', 'economy', 'business', 'congress'] #omitted going w4, for w7 business

# How to color the points in the plot. The other option is "period" for time period
color_by = 'generation'
# Get all word positions
word_positions = get_word_positions(keywords)

# Get all contexts around the words
keyword_contexts = []
keyword_contexts_tokens = []
words = []

for position in word_positions:
  words.append(word_lookup[all_word_ids[position]])
  keyword_contexts.append(get_context_clean(position))
  keyword_contexts_tokens.append(get_context(position))

# Reduce word vectors with PCA
pca = PCA(n_components=2)
pca.fit(all_word_vectors[word_positions,:].T)

# Make a DataFrame with PCA results
df = pd.DataFrame({"x": pca.components_[0,:], "y": pca.components_[1,:],
                   "context": keyword_contexts, "tokens": keyword_contexts_tokens, "word": words})
# Match original text and metadata
df[['title', 'speaker', 'generation', 'citation']] = df.apply(find_original_speech, axis='columns', result_type='expand')

# Rename columns so that the context shows up as the "title" in the tooltip (bigger and bolded)
df = df.rename(columns={'title': 'title', 'context': 'speech_title'})

# Make the plot
alt.Chart(df, title=f"Word Similarity: {', '.join(keywords).title()}").mark_circle(size=200).encode(
    alt.X('x',
        scale=alt.Scale(zero=False)
    ), y="y",
    color= color_by,
    tooltip=['speech_title', 'citation', 'title', 'speaker', 'generation']
    ).interactive().properties(
    width=500,
    height=500
)

# List of keywords that you want to compare
# we are doing top five words in each topic for five topics (based on which we found different) 
# based on t11 w2
keywords = ['jobs']

# How to color the points in the plot. The other option is "period" for time period
color_by = 'generation'
# Get all word positions
word_positions = get_word_positions(keywords)

# Get all contexts around the words
keyword_contexts = []
keyword_contexts_tokens = []
words = []

for position in word_positions:
  words.append(word_lookup[all_word_ids[position]])
  keyword_contexts.append(get_context_clean(position))
  keyword_contexts_tokens.append(get_context(position))

# Reduce word vectors with PCA
pca = PCA(n_components=2)
pca.fit(all_word_vectors[word_positions,:].T)

# Make a DataFrame with PCA results
df = pd.DataFrame({"x": pca.components_[0,:], "y": pca.components_[1,:],
                   "context": keyword_contexts, "tokens": keyword_contexts_tokens, "word": words})
# Match original text and metadata
df[['title', 'speaker', 'generation', 'citation']] = df.apply(find_original_speech, axis='columns', result_type='expand')

# Rename columns so that the context shows up as the "title" in the tooltip (bigger and bolded)
df = df.rename(columns={'title': 'title', 'context': 'speech_title'})

# Make the plot
alt.Chart(df, title=f"Word Similarity: {', '.join(keywords).title()}").mark_circle(size=200).encode(
    alt.X('x',
        scale=alt.Scale(zero=False)
    ), y="y",
    color= color_by,
    tooltip=['speech_title', 'citation', 'title', 'speaker', 'generation']
    ).interactive().properties(
    width=500,
    height=500
)

# List of keywords that you want to compare
# we are doing top five words in each topic for five topics (based on which we found different) 
# based on t11 w2
keywords = ['tax']

# How to color the points in the plot. The other option is "period" for time period
color_by = 'generation'
# Get all word positions
word_positions = get_word_positions(keywords)

# Get all contexts around the words
keyword_contexts = []
keyword_contexts_tokens = []
words = []

for position in word_positions:
  words.append(word_lookup[all_word_ids[position]])
  keyword_contexts.append(get_context_clean(position))
  keyword_contexts_tokens.append(get_context(position))

# Reduce word vectors with PCA
pca = PCA(n_components=2)
pca.fit(all_word_vectors[word_positions,:].T)

# Make a DataFrame with PCA results
df = pd.DataFrame({"x": pca.components_[0,:], "y": pca.components_[1,:],
                   "context": keyword_contexts, "tokens": keyword_contexts_tokens, "word": words})
# Match original text and metadata
df[['title', 'speaker', 'generation', 'citation']] = df.apply(find_original_speech, axis='columns', result_type='expand')

# Rename columns so that the context shows up as the "title" in the tooltip (bigger and bolded)
df = df.rename(columns={'title': 'title', 'context': 'speech_title'})

# Make the plot
alt.Chart(df, title=f"Word Similarity: {', '.join(keywords).title()}").mark_circle(size=200).encode(
    alt.X('x',
        scale=alt.Scale(zero=False)
    ), y="y",
    color= color_by,
    tooltip=['speech_title', 'citation', 'title', 'speaker', 'generation']
    ).interactive().properties(
    width=500,
    height=500
)

# List of keywords that you want to compare
# we are doing top five words in each topic for five topics (based on which we found different) 
# based on t11 w3
keywords = ['economy']

# How to color the points in the plot. The other option is "period" for time period
color_by = 'generation'
# Get all word positions
word_positions = get_word_positions(keywords)

# Get all contexts around the words
keyword_contexts = []
keyword_contexts_tokens = []
words = []

for position in word_positions:
  words.append(word_lookup[all_word_ids[position]])
  keyword_contexts.append(get_context_clean(position))
  keyword_contexts_tokens.append(get_context(position))

# Reduce word vectors with PCA
pca = PCA(n_components=2)
pca.fit(all_word_vectors[word_positions,:].T)

# Make a DataFrame with PCA results
df = pd.DataFrame({"x": pca.components_[0,:], "y": pca.components_[1,:],
                   "context": keyword_contexts, "tokens": keyword_contexts_tokens, "word": words})
# Match original text and metadata
df[['title', 'speaker', 'generation', 'citation']] = df.apply(find_original_speech, axis='columns', result_type='expand')

# Rename columns so that the context shows up as the "title" in the tooltip (bigger and bolded)
df = df.rename(columns={'title': 'title', 'context': 'speech_title'})

# Make the plot
alt.Chart(df, title=f"Word Similarity: {', '.join(keywords).title()}").mark_circle(size=200).encode(
    alt.X('x',
        scale=alt.Scale(zero=False)
    ), y="y",
    color= color_by,
    tooltip=['speech_title', 'citation', 'title', 'speaker', 'generation']
    ).interactive().properties(
    width=500,
    height=500
)

# List of keywords that you want to compare
# we are doing top five words in each topic for five topics (based on which we found different) 
# based on t12, government topics
keywords = ['government', 'think', 'congress', 'vice', 'public', 'policy'] #added w10 "policy" bc iffy about "think"

# How to color the points in the plot. The other option is "period" for time period
color_by = 'word'
# Get all word positions
word_positions = get_word_positions(keywords)

# Get all contexts around the words
keyword_contexts = []
keyword_contexts_tokens = []
words = []

for position in word_positions:
  words.append(word_lookup[all_word_ids[position]])
  keyword_contexts.append(get_context_clean(position))
  keyword_contexts_tokens.append(get_context(position))

# Reduce word vectors with PCA
pca = PCA(n_components=2)
pca.fit(all_word_vectors[word_positions,:].T)

# Make a DataFrame with PCA results
df = pd.DataFrame({"x": pca.components_[0,:], "y": pca.components_[1,:],
                   "context": keyword_contexts, "tokens": keyword_contexts_tokens, "word": words})
# Match original text and metadata
df[['title', 'speaker', 'generation', 'citation']] = df.apply(find_original_speech, axis='columns', result_type='expand')

# Rename columns so that the context shows up as the "title" in the tooltip (bigger and bolded)
df = df.rename(columns={'title': 'title', 'context': 'speech_title'})

# Make the plot
alt.Chart(df, title=f"Word Similarity: {', '.join(keywords).title()}").mark_circle(size=200).encode(
    alt.X('x',
        scale=alt.Scale(zero=False)
    ), y="y",
    color= color_by,
    tooltip=['speech_title', 'citation', 'title', 'speaker', 'generation']
    ).interactive().properties(
    width=500,
    height=500
)

# List of keywords that you want to compare
# we are doing top five words in each topic for five topics (based on which we found different) 
# based on t12, government topic generarion
keywords = ['government', 'think', 'congress', 'vice', 'public', 'policy'] #added w10 "policy" bc iffy about "think"

# How to color the points in the plot. The other option is "period" for time period
color_by = 'generation'
# Get all word positions
word_positions = get_word_positions(keywords)

# Get all contexts around the words
keyword_contexts = []
keyword_contexts_tokens = []
words = []

for position in word_positions:
  words.append(word_lookup[all_word_ids[position]])
  keyword_contexts.append(get_context_clean(position))
  keyword_contexts_tokens.append(get_context(position))

# Reduce word vectors with PCA
pca = PCA(n_components=2)
pca.fit(all_word_vectors[word_positions,:].T)

# Make a DataFrame with PCA results
df = pd.DataFrame({"x": pca.components_[0,:], "y": pca.components_[1,:],
                   "context": keyword_contexts, "tokens": keyword_contexts_tokens, "word": words})
# Match original text and metadata
df[['title', 'speaker', 'generation', 'citation']] = df.apply(find_original_speech, axis='columns', result_type='expand')

# Rename columns so that the context shows up as the "title" in the tooltip (bigger and bolded)
df = df.rename(columns={'title': 'title', 'context': 'speech_title'})

# Make the plot
alt.Chart(df, title=f"Word Similarity: {', '.join(keywords).title()}").mark_circle(size=200).encode(
    alt.X('x',
        scale=alt.Scale(zero=False)
    ), y="y",
    color= color_by,
    tooltip=['speech_title', 'citation', 'title', 'speaker', 'generation']
    ).interactive().properties(
    width=500,
    height=500
)

# List of keywords that you want to compare
# we are doing top five words in each topic for five topics (based on which we found different) 
# based on t12 w1
keywords = ['government']

# How to color the points in the plot. The other option is "period" for time period
color_by = 'generation'
# Get all word positions
word_positions = get_word_positions(keywords)

# Get all contexts around the words
keyword_contexts = []
keyword_contexts_tokens = []
words = []

for position in word_positions:
  words.append(word_lookup[all_word_ids[position]])
  keyword_contexts.append(get_context_clean(position))
  keyword_contexts_tokens.append(get_context(position))

# Reduce word vectors with PCA
pca = PCA(n_components=2)
pca.fit(all_word_vectors[word_positions,:].T)

# Make a DataFrame with PCA results
df = pd.DataFrame({"x": pca.components_[0,:], "y": pca.components_[1,:],
                   "context": keyword_contexts, "tokens": keyword_contexts_tokens, "word": words})
# Match original text and metadata
df[['title', 'speaker', 'generation', 'citation']] = df.apply(find_original_speech, axis='columns', result_type='expand')

# Rename columns so that the context shows up as the "title" in the tooltip (bigger and bolded)
df = df.rename(columns={'title': 'title', 'context': 'speech_title'})

# Make the plot
alt.Chart(df, title=f"Word Similarity: {', '.join(keywords).title()}").mark_circle(size=200).encode(
    alt.X('x',
        scale=alt.Scale(zero=False)
    ), y="y",
    color= color_by,
    tooltip=['speech_title', 'citation', 'title', 'speaker', 'generation']
    ).interactive().properties(
    width=500,
    height=500
)

# List of keywords that you want to compare
# we are doing top five words in each topic for five topics (based on which we found different) 
# based on t12 w2
keywords = ['think']

# How to color the points in the plot. The other option is "period" for time period
color_by = 'generation'
# Get all word positions
word_positions = get_word_positions(keywords)

# Get all contexts around the words
keyword_contexts = []
keyword_contexts_tokens = []
words = []

for position in word_positions:
  words.append(word_lookup[all_word_ids[position]])
  keyword_contexts.append(get_context_clean(position))
  keyword_contexts_tokens.append(get_context(position))

# Reduce word vectors with PCA
pca = PCA(n_components=2)
pca.fit(all_word_vectors[word_positions,:].T)

# Make a DataFrame with PCA results
df = pd.DataFrame({"x": pca.components_[0,:], "y": pca.components_[1,:],
                   "context": keyword_contexts, "tokens": keyword_contexts_tokens, "word": words})
# Match original text and metadata
df[['title', 'speaker', 'generation', 'citation']] = df.apply(find_original_speech, axis='columns', result_type='expand')

# Rename columns so that the context shows up as the "title" in the tooltip (bigger and bolded)
df = df.rename(columns={'title': 'title', 'context': 'speech_title'})

# Make the plot
alt.Chart(df, title=f"Word Similarity: {', '.join(keywords).title()}").mark_circle(size=200).encode(
    alt.X('x',
        scale=alt.Scale(zero=False)
    ), y="y",
    color= color_by,
    tooltip=['speech_title', 'citation', 'title', 'speaker', 'generation']
    ).interactive().properties(
    width=500,
    height=500
)

# List of keywords that you want to compare
# we are doing top five words in each topic for five topics (based on which we found different) 
# based on t12 w3
keywords = ['congress']

# How to color the points in the plot. The other option is "period" for time period
color_by = 'generation'
# Get all word positions
word_positions = get_word_positions(keywords)

# Get all contexts around the words
keyword_contexts = []
keyword_contexts_tokens = []
words = []

for position in word_positions:
  words.append(word_lookup[all_word_ids[position]])
  keyword_contexts.append(get_context_clean(position))
  keyword_contexts_tokens.append(get_context(position))

# Reduce word vectors with PCA
pca = PCA(n_components=2)
pca.fit(all_word_vectors[word_positions,:].T)

# Make a DataFrame with PCA results
df = pd.DataFrame({"x": pca.components_[0,:], "y": pca.components_[1,:],
                   "context": keyword_contexts, "tokens": keyword_contexts_tokens, "word": words})
# Match original text and metadata
df[['title', 'speaker', 'generation', 'citation']] = df.apply(find_original_speech, axis='columns', result_type='expand')

# Rename columns so that the context shows up as the "title" in the tooltip (bigger and bolded)
df = df.rename(columns={'title': 'title', 'context': 'speech_title'})

# Make the plot
alt.Chart(df, title=f"Word Similarity: {', '.join(keywords).title()}").mark_circle(size=200).encode(
    alt.X('x',
        scale=alt.Scale(zero=False)
    ), y="y",
    color= color_by,
    tooltip=['speech_title', 'citation', 'title', 'speaker', 'generation']
    ).interactive().properties(
    width=500,
    height=500
)

df.to_csv('bert-word-ring.csv', index=False, encoding='utf-8')