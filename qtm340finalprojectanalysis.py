# -*- coding: utf-8 -*-
"""QTM340FinalProjectAnalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ydkHfOARhYYrXGx_H_LSFwDI8OhXSUre

Research question: In all spoken remarks made by U.S. presidents, what topics were most prevelant and how did the topics change over time?

Import libraries
"""

import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns

# import and setup modules we'll be using in this notebook
import logging # for logging status etc
import itertools # helpful library for iterating through things

import numpy as np # this is a powerful python math package that many others are based on
import gensim # our topic modeling library
import os # for file i/o

from matplotlib.pyplot import figure

# configure logging, since topic modeling takes a while and it's good to know what's going on 
logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)
logging.root.level = logging.INFO  

# a helpful function that returns the first `n` elements of the stream as plain list.
# we'll use this later
def head(stream, n=10):
    return list(itertools.islice(stream, n))

# import some more gensim modules for processing the corpus
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS

from gensim.models.wrappers import LdaMallet

"""Load and process the data"""

from google.colab import drive
drive.mount('/content/drive')

# Used these lines to create docs in doc directory
df1 = pd.read_csv('/content/drive/MyDrive/QTM340 Final Project/presidential_speeches_1.csv', encoding = 'utf-8', sep = '+')
df2 = pd.read_csv('/content/drive/MyDrive/QTM340 Final Project/presidential_speeches_2.csv', encoding = 'utf-8', sep = '+')
df3 = pd.read_csv('/content/drive/MyDrive/QTM340 Final Project/presidential_speeches_3.csv', encoding = 'utf-8', sep = '+')

# DO NOT RUN DO NOT RUN DO NOT RUN
'''
# Used these lines to create docs in doc directory - DO NOT RUN
basedir = '/content/drive/MyDrive/QTM340 Final Project/docs/'
for index1, row in df1.iterrows():
  text = row['text']
  file_name = basedir + str(index1) +  '.txt'    
  with open(file_name, 'w') as f:
    f.write(text)

for index2, row in df2.iterrows():
  text = row['text']
  file_name = basedir + str(index1 + index2 + 1) +  '.txt'    
  with open(file_name, 'w') as f:
    f.write(text)

for index3, row in df3.iterrows():
  text = row['text']
  file_name = basedir + str(index1 + index2 + 1 + index3 + 1) +  '.txt'    
  with open(file_name, 'w') as f:
    f.write(text)
'''

# The index of the speech in this dataframe is equivalent to the title of the speech in the "docs" folder.
# E.g. George Washington's inaugural address on April 30, 1789 is in the "docs" folder as "0.txt"
df = pd.concat([df1, df2, df3])
df = df.reset_index(drop=True)
df

pd.DataFrame(df.groupby(['speaker'])['speaker'].count().sort_values(ascending = False)).to_csv('blah.csv')

df['datetime'] = pd.to_datetime(df['date'])

# create a list of our conditions
conditions = [
    (df['datetime'] < pd.Timestamp(1993, 1, 20)),
    ((df['datetime'] >= pd.Timestamp(1993, 1, 20)) & (df['datetime'] < pd.Timestamp(2009, 1, 20))), # george bush and bill clinton
    (df['datetime'] >= pd.Timestamp(2009, 1, 20)) # obama, trump, biden
    ]

# create a list of the values we want to assign for each condition
values = ['before_1993', '1993_2009', '2009_present']

# create a new column and use np.select to assign values to it using our lists as arguments
df['generation'] = np.select(conditions, values)

df['generation'].value_counts().plot(kind = 'bar')
plt.title('Documents per Time Period')
plt.xlabel('Time Period')
plt.ylabel('Number of Documents')

test = df[(df['generation'] == '1993_2009') & (df['speaker'] != 'George W. Bush')]
test['speaker'].value_counts()

df['year'] = pd.DatetimeIndex(df['datetime']).year

df['year'].value_counts().sort_index()

figure(figsize=(6, 15), dpi=80)
h = df['year'].value_counts().sort_index().plot(kind='barh')
plt.title("Number of Documents Per Year")
plt.ylabel("Year")
plt.xlabel("Number of Documents")
plt.rcParams.update({'font.size': 8})

# this defines our tokenize function for future use
def tokenize(text):
    return [token for token in simple_preprocess(text) if token not in STOPWORDS]

# A function to yield each doc in a base directory as a `(filename, tokens)` tuple.

def iter_docs(base_dir):
    docCount = 0
    docs = os.listdir(base_dir)

    for doc in docs:
        if not doc.startswith('.'):
            with open(base_dir + doc, "r") as file:
                text = file.read()
                tokens = tokenize(text) 
        
                yield doc, tokens

# set up the stream (this cell takes a couple min to run)
basedir = '/content/drive/MyDrive/QTM340 Final Project/docs/'
stream = iter_docs(basedir)

# all of the rest is standard from the gensim documentation
doc_stream = (tokens for _, tokens in stream)
              
id2word_pres = gensim.corpora.Dictionary(doc_stream) 

print(id2word_pres)

# Store dictionary to Drive
#id2word_pres.save('/content/drive/MyDrive/QTM340 Final Project/pres_all.dictionary')

# Load in dictionary from drive
id2word_pres = gensim.corpora.Dictionary.load('/content/drive/My Drive/QTM340 Final Project/pres_all.dictionary')

# This line below would, for example, filter out 50 most frequent words.
# It's commented out here because I don't want to use it in this case,
# but it's handy to know about. 
id2word_pres.filter_n_most_frequent(20)

# this line filters out words that appear only 1 doc, keeping the rest (this is relevant bc of the assumptions of the algorithm)
# I will use this one 
# Note how no_below and no_above take different data types. Not sure why! 
id2word_pres.filter_extremes(no_below=2, no_above=1.0)

id2word_pres

# a class we need; this is the same for every topic model you create with gensim. 
# no need to modify it here

class Corpus(object):
    def __init__(self, dump_file, dictionary, clip_docs=None):
        self.dump_file = dump_file
        self.dictionary = dictionary
        self.clip_docs = clip_docs
    
    def __iter__(self):
        self.titles = []
        for title, tokens in itertools.islice(iter_docs(self.dump_file), self.clip_docs):
            self.titles.append(title)
            yield self.dictionary.doc2bow(tokens)
    
    def __len__(self):
        return self.clip_docs

# create a stream of bag-of-words vectors
# here's another place where you'd change the name/location of the corpus if you want to
# run a topic model on something else
pres_corpus = Corpus('/content/drive/MyDrive/QTM340 Final Project/docs/', id2word_pres)
pres_corpus

from gensim.corpora import MmCorpus
# store corpus to Drive 
MmCorpus.serialize('/content/drive/My Drive/QTM340 Final Project/pres_corpus_all.mm', pres_corpus)

# Commented out IPython magic to ensure Python compatibility.
# %time lda_model = gensim.models.LdaModel(pres_corpus, num_topics=15, id2word=id2word_pres, passes=100)

# Store model to Drive 
lda_model.save('/content/drive/My Drive/QTM340 Final Project/lda_all_pres_15topics_100iters.model')

# Load in model from drive
lda_model = gensim.models.LdaModel.load('/content/drive/My Drive/QTM340 Final Project/lda_all_pres_15topics_100iters.model')

from gensim.models.coherencemodel import CoherenceModel

cm = CoherenceModel(model=lda_model, corpus=pres_corpus, coherence='u_mass')

coherence = cm.get_coherence()  # get coherence value

coherence

"""Explore the results"""

# gensim comes with a bunch of functions that make interacting with the output of the topic
# model a little easier. this one shows the topics. 

# show the topics, in the format (number of topics to show, number of terms)
# note that all words are in all topics, just some topics consist of very very small
# proportions of that word


lda_model.show_topics(15, 20)

# let's format the words a little more nicely; 
# the formatted=False parameter returns tuples of (word, probability)

topics = lda_model.show_topics(20, 20, formatted=False)

for topic in topics:
    topic_num = topic[0]
    topic_words = ""
    
    topic_pairs = topic[1]
    for pair in topic_pairs:
        topic_words += pair[0] + ", "
    
    print("T" + str(topic_num) + ": " + topic_words)

#speaker, T0, T1, T2, T3, T4, T5, T6, T7, T8, T9, T10, T11, T12, T13, T14, T15, T16, T17, T18, T19  = ([0]*len(df['speaker'].unique()) for i in range(21))
speaker, T0, T1, T2, T3, T4, T5, T6, T7, T8, T9, T10, T11, T12, T13, T14  = ([0]*len(df['speaker'].unique()) for i in range(16))

pres = 0
for s in df['speaker'].unique():
  df_s = df[df['speaker'] == s]
  speaker[pres] = s
  print(s)

  for index, row in df_s.iterrows():
    text = row['text']
    tokens = tokenize(text)

    # create the bag of words for the document on the basis of the Wheel dictionary, created above
    doc_bow = id2word_pres.doc2bow(tokens)

    # get the topics that the doc consists of
    doc_topics = lda_model.get_document_topics(doc_bow)

    for t in doc_topics:
      if(t[0] == 0):
        T0[pres] += t[1] / len(df_s)
      if(t[0] == 1):
        T1[pres] += t[1] / len(df_s)
      if(t[0] == 2):
        T2[pres] += t[1] / len(df_s)
      if(t[0] == 3):
        T3[pres] += t[1] / len(df_s)
      if(t[0] == 4):
        T4[pres] += t[1] / len(df_s)
      if(t[0] == 5):
        T5[pres] += t[1] / len(df_s)
      if(t[0] == 6):
        T6[pres] += t[1] / len(df_s)
      if(t[0] == 7):
        T7[pres] += t[1] / len(df_s)
      if(t[0] == 8):
        T8[pres] += t[1] / len(df_s)
      if(t[0] == 9):
        T9[pres] += t[1] / len(df_s)
      if(t[0] == 10):
        T10[pres] += t[1] / len(df_s)
      if(t[0] == 11):
        T11[pres] += t[1] / len(df_s)
      if(t[0] == 12):
        T12[pres] += t[1] / len(df_s)
      if(t[0] == 13):
        T13[pres] += t[1] / len(df_s)
      if(t[0] == 14):
        T14[pres] += t[1] / len(df_s)
      '''
      if(t[0] == 15):
        T15[pres] += t[1] / len(df_s)
      if(t[0] == 16):
        T16[pres] += t[1] / len(df_s)
      if(t[0] == 17):
        T17[pres] += t[1] / len(df_s)
      if(t[0] == 18):
        T18[pres] += t[1] / len(df_s)
      if(t[0] == 19):
        T19[pres] += t[1] / len(df_s)
      '''

  pres += 1

pres_topics_props = pd.DataFrame({'Speaker': speaker,
                                  'T0': T0,
                                  'T1': T1,
                                  'T2': T2,
                                  'T3': T3,
                                  'T4': T4,
                                  'T5': T5,
                                  'T6': T6,
                                  'T7': T7,
                                  'T8': T8,
                                  'T9': T9,
                                  'T10': T10,
                                  'T11': T11,
                                  'T12': T12,
                                  'T13': T13,
                                  'T14': T14
                                  #'T15': T15,
                                  #'T16': T16,
                                  #'T17': T17,
                                  #'T18': T18,
                                  #'T19': T19,
                                  } )
pres_topics_props

import seaborn as sns

gw = pres_topics_props.drop(['T14', 'T9', 'T8', 'T5', 'T1'], axis = 1).iloc[44, 1:] # first row, ignore speaker colmn
index = gw.index
values = gw.values
plt.rcParams.update({'font.size': 15})
figure(figsize=(8, 5), dpi=80)
sns.barplot(index, values)
plt.title("Michelle Obama's Topic Proportions")
plt.xlabel("Topic")
plt.ylabel("Average Proportion")

pres_topics_props_2 = pres_topics_props.copy()
pres_topics_props_2.index = pres_topics_props_2['Speaker']
pres_topics_props_2

plt.rcParams["figure.figsize"] = (15, 8)
plt.rcParams.update({'font.size': 15})
pres_topics_props_2.drop(['T14', 'T9', 'T8', 'T5', 'T1'], axis = 1).plot(kind = 'bar', stacked = True)
plt.legend(bbox_to_anchor=(1.04, 1), loc="upper left")
plt.ylabel('Average Proportion of Each Topic')
plt.title("Average Proportion of Topic in Each Speaker's Documents")

"""# Old analysis"""

dict_topics = {0: 'Budget, Taxes, Cut',
               1: 'People, Jobs, Workers',
               2: 'Health Care',
               3: 'Ukraine, Russia, Iran, Nuclear, Continue',
               4: 'COVID-19',
               5: 'Military, Veterans, Thanks',
               6: 'China, Economy, Global, Japan, India', 
               7: 'Irag, War, Afghanistan, Terrorists',
               8: 'War, Peace, Great, Power',
               9: 'Applause, Barack Obama, Christmas',
               10: 'Thanks, White, Great, President',
               11: 'Applause, Senate, Congress, Bush',
               12: 'Preace, Freedom, United',
               13: 'United Nations, Democracy, Freedom',
               14: 'Laughter, Young, Kids',
               15: 'Border, Police, Drug, Violence',
               16: 'Energy, Clean, Oil',
               17: 'People, Want, Election',
               18: 'President, Thanks, Trump',
               19: 'School, Education, Teachers',
               }

dict_topics

dict_closer = {'economy': 'T0, T1, T6',
               'war': 'T3, T5, T7, T8',
               'gratitude': 'T9, T10, T11, T18',
}
 
dict_closer

"""The topics in our corpus are issues surrounding the economy, war in both a positive and negative light, and gratitude, specifically towards the President of the United States. Other issues include health care, COVID-19, inter-state violence, the war on drugs and border violence, clean energy, and education. 

We planned to look into the sentiment of each topic. Adding sentiment analysis may benefit these findings because the same subject is associated with different adjectives. For example, issues related to the economy include T0, T1, and T6. T0 and T1 are solely on domestic matters, whereas T6 ties in globalization. A sentiment analysis comparing these groups would show how President speaks about other countries. Comparing T0 and T1 would show the sentiment of the economy as a whole versus the workers. 

The differences between topics 3,5,7, and 8 indicate a difference in perspective toward war. Topic 3 is the only one with the word continue and has no associated adjectives. This differs from 5 and 8, which are both reflective and seem to have a positive sentiment. Topic 7 indicates the violence and negative associations one could have surrounding war. 

Besides sentiment analysis, we could do further research by splitting the topics by President or time period. One, we could identify which presidents cared about which topics. By doing so, we could group the issues by party affiliation. Two, COVID overwhelms our corpus even though it dates back to speeches made by George Washington. Dividing our corpus by time period will allow us to claim how topics have changed over time properly. 
"""