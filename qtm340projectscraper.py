# -*- coding: utf-8 -*-
"""QTM340ProjectScraper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h1NWES5_-YbHoHS3hLpz_ALUTj8q4kgb

# Get Miller center speeches dataset

Scrape the Miller Center for presidential speeches containing our presidents of interest, [list presidents]. Code adapted from: https://gist.github.com/kboghe/4a0aa1b656a1be0674d765ce0a0c3542
"""

pip install dateparser

pip install selenium

from bs4 import BeautifulSoup as bs
import pandas as pd
import time
import selenium
from selenium import webdriver
import re
import dateparser

!apt-get update
!apt install chromium-chromedriver
!cp /usr/lib/chromium-browser/chromedriver /usr/bin
!pip install selenium
# set options to be headless, ..
from selenium import webdriver
options = webdriver.ChromeOptions()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')

options.add_argument("--start-maximized");
options.add_argument("--window-size=1920,1080");

#start Chromedriver and access MillerCenter website
driver = webdriver.Chrome('chromedriver', options = options)
url = "https://millercenter.org/the-presidency/presidential-speeches"
driver.get(url)
driver.implicitly_wait(10)

#keep scrolling down until page stops loading additional records or reaches time limit (time limit is useful bc we're not looking at a super big range)
pause_scroll = 6
last_try = 0
initialcoord = driver.execute_script("return document.body.scrollHeight")
end_time = time.time() + (60 * 1)
while True and time.time() <= end_time:
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(pause_scroll)
    newcoord = driver.execute_script("return document.body.scrollHeight")
    if newcoord == initialcoord:
        time.sleep(20)
        last_try = last_try + 1
    if last_try > 1:
        break
    newcoord = initialcoord

#retrieve urls to all speeches
page_source = driver.page_source
bsobject_linkpage = bs(page_source,'lxml')
links = bsobject_linkpage.find_all("a", href= re.compile('presidential-speeches/'))
link_list = list()
for link in links:
    link_specific = link['href']
    link_list.append("https://millercenter.org"+link_specific)

link_list_old = link_list.copy()
link_list = link_list_old[0:200] # change this later to speeches we want
link_list

from selenium.common.exceptions import NoSuchElementException

#scrape the speech#
from selenium.webdriver.common.by import By
title, speech, name, date, about = ([] for i in range(5))
for index,link in enumerate(link_list):
  try:
    #access speech page with Selenium and load html source into Beautifulsoup#
    driver.get(link_list[index])
    driver.find_element(By.CSS_SELECTOR, 'div[class="transcript-inner"]')
    page_source = driver.page_source
    bsobject_speechpage = bs(page_source, 'lxml')

    #scrape speech and other properties#
    title.append((bsobject_speechpage.find('h2', class_="presidential-speeches--title").text).rstrip())
    try:
        speech_raw = bsobject_speechpage.find('div', class_="transcript-inner").text
    except:
        speech_raw = (bsobject_speechpage.find('div', class_="view-transcript").text).rstrip()
    speech.append(re.sub("Transcript|\\n"," ",speech_raw))
    name.append((bsobject_speechpage.find('p', class_="president-name").text).rstrip())
    date.append((dateparser.parse(bsobject_speechpage.find('p', class_="episode-date").text)))
    try:
        about_raw = bsobject_speechpage.find('div', class_="about-sidebar--intro").text.rstrip()
    except:
        about_raw = about.append("No info available")
    empty = ""
    if about_raw == empty or about_raw is None:
        about_raw = "No info available"
    about.append(re.sub("\\n"," ",about_raw))
  except NoSuchElementException:
    print('Cant access: '+ link)
    pass

len(title)

len(name)

len(about)

len(speech)

len(date)

#save this to a dataframe and save to a csv file#
#speeches_presidents = pd.DataFrame({'name':name,'title':title,'date':date,'info':about,'speech':speech}, columns=['name','title','date','info','speech'])
speeches_presidents = pd.DataFrame({'name':name,'title':title,'date':date,'speech':speech}, columns=['name','title','date','speech'])
speeches_presidents['speech'] = speeches_presidents['speech'].apply(lambda x: x.replace(".",". "))
speeches_presidents.to_csv("speeches.csv", encoding="utf-8",quotechar="'",index=False)

speeches_presidents

pd.set_option('display.max_colwidth', None)
print(speeches_presidents[(speeches_presidents['name'] == 'Joe Biden') & (speeches_presidents['date'] == '2022-09-01')]['speech'])

"""# Get UCSB / American Presidency Project speeches dataset
https://www.presidency.ucsb.edu/documents/app-categories/elections-and-transitions/campaign-documents

Code adapted from https://github.com/milesmcc/app-scraper
"""

from bs4 import BeautifulSoup
import requests
import time

# set to your search url page
#SEARCH_INITIAL_PAGE = "https://www.presidency.ucsb.edu/advanced-search?field-keywords=&field-keywords2=NATO%20%22North%20Atlantic%20Treaty%20Organization%22&field-keywords3=&from%5Bdate%5D=&to%5Bdate%5D=&person2=&items_per_page=100"
#SEARCH_INITIAL_PAGE = "https://www.presidency.ucsb.edu/advanced-search?field-keywords=&field-keywords2=&field-keywords3=&from%5Bdate%5D=01-01-2014&to%5Bdate%5D=11-15-2016&person2=200301&category2%5B%5D=63&category2%5B%5D=10&items_per_page=100"
# trump documents with category "campaign docs" or "elections and transitions", from jan 1 2014 to nov 15 2016 ( need to repeat this process for other presidents/timeframes of interest)
#https://www.presidency.ucsb.edu/advanced-search?field-keywords=&field-keywords2=&field-keywords3=&from%5Bdate%5D=&to%5Bdate%5D=&person2=&category2%5B%5D=8&items_per_page=100

#SEARCH_INITIAL_PAGE = "https://www.presidency.ucsb.edu/advanced-search?field-keywords=&field-keywords2=&field-keywords3=&from%5Bdate%5D=01-01-1789&to%5Bdate%5D=01-01-2005&person2=&category2%5B%5D=8&items_per_page=100"

#SEARCH_INITIAL_PAGE = "https://www.presidency.ucsb.edu/advanced-search?field-keywords=&field-keywords2=&field-keywords3=&from%5Bdate%5D=01-01-2005&to%5Bdate%5D=11-03-2022&person2=&category2%5B%5D=8&items_per_page=100"

#SEARCH_INITIAL_PAGE = "https://www.presidency.ucsb.edu/advanced-search?field-keywords=&field-keywords2=&field-keywords3=&from%5Bdate%5D=01-01-2005&to%5Bdate%5D=11-03-2014&person2=&category2%5B%5D=8&items_per_page=100"
SEARCH_INITIAL_PAGE = "https://www.presidency.ucsb.edu/advanced-search?field-keywords=&field-keywords2=&field-keywords3=&from%5Bdate%5D=11-03-2014&to%5Bdate%5D=11-03-2022&person2=&category2%5B%5D=8&items_per_page=100"

#SEARCH_INITIAL_PAGE = "https://www.presidency.ucsb.edu/advanced-search?field-keywords=&field-keywords2=&field-keywords3=&from%5Bdate%5D=&to%5Bdate%5D=&person2=&category2%5B%5D=8&items_per_page=100"


# seconds between queries
TIME_BETWEEN_QUERIES = 0 # be nice!

search_result_page = SEARCH_INITIAL_PAGE
documents = []

# find all documents
while search_result_page != None:
    print("Loading results from %s..." % search_result_page)
    page = requests.get(search_result_page)
    soup = BeautifulSoup(page.text, "html.parser")
    for link_block in soup.find_all("tr", {"class": ["even", "odd"]}):
        link = "https://www.presidency.ucsb.edu" + link_block.find("td", {"class": "views-field-title"}).find("a")["href"]
        print("Found link: %s" % link)
        documents.append(link)
    search_result_page_link = soup.find("a", {"title": "Go to next page"})
    if search_result_page_link != None:
        search_result_page = "https://www.presidency.ucsb.edu" + search_result_page_link["href"]
    else:
        search_result_page = None
    time.sleep(TIME_BETWEEN_QUERIES)

with open("documents.txt", "w") as outfile:
    for document in documents:
        outfile.write(document + "\n")

print("Found %s total documents." % str(len(documents)))

import json
from tqdm import tqdm

# set document file
DOCUMENTS_FILE = "documents.txt"
OUTPUTS_FILE = "document_contents.json"

# seconds between queries
TIME_BETWEEN_QUERIES = 0 # be nice!

print("Loading documents...")

documents = {}

try:
    with open(OUTPUTS_FILE, "r") as infile:
        documents = json.load(infile)
except:
    pass

document_links = []

with open(DOCUMENTS_FILE, "r") as infile:
    document_links = [link for link in infile.readlines() if link not in documents]

print("Loaded %s document links to process..." % str(len(document_links)))

textc, datec, titlec, speakerc, citationc = ([] for i in range(5))
for document_link in tqdm(document_links):
    page = requests.get(document_link)
    soup = BeautifulSoup(page.text, "html.parser")
    if soup.find("div", {"class": "field-docs-content"}) is not None:
      text = soup.find("div", {"class": "field-docs-content"}).text
      date = soup.find("span", {"class": "date-display-single"}).text
      title = soup.find("div", {"class": "field-ds-doc-title"}).text
      speaker = soup.find("h3", {"class": "diet-title"}).text
      citation = soup.find("p", {"class": "ucsbapp_citation"}).text
      documents[document_link] = {
          "text": text,
          "date": date,
          "title": title,
          "speaker": speaker,
          "citation": citation
      }
      textc.append(text)
      datec.append(date)
      titlec.append(title)
      speakerc.append(speaker)
      citationc.append(citation)

    # temporary save
    with open(OUTPUTS_FILE, "w") as outfile:
        json.dump(documents, outfile)

    time.sleep(TIME_BETWEEN_QUERIES)

#save this to a dataframe and save to a csv file#
import pandas as pd
elections_presidents = pd.DataFrame({'title':titlec,'date':datec,'speaker':speakerc,'text':textc,'citation': citationc}, columns=['title','date','speaker','text', 'citation'])

elections_presidents.to_csv("presidential_speeches_3.csv", encoding="utf-8",sep = '+',index=False)

elections_presidents

from google.colab import drive
drive.mount('/content/drive')

from google.colab import files

!cp document_contents.json "/content/drive/MyDrive/"

from google.colab import drive
drive.mount('/content/drive')